<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Flink实战（一）：订单支付实时监控</title>
    <url>/2019/12/17/Flink%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E8%AE%A2%E5%8D%95%E6%94%AF%E4%BB%98%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7/</url>
    <content><![CDATA[<h4 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h4><p>​        在电商平台中，最终创造收入和利润的是用户下单购买的环节；更具体一点，是用户真正完成支付动作的时候。用户下单的行为可以表明用户对商品的需求，但在现实中，并不是每次下单都会被用户立刻支付。当拖延一段时间后，用户支付的意愿会降低。所以为了让用户更有紧迫感从而提高支付转化率，同时也为了防范订单支付环节的安全风险，电商网站往往会对订单状态进行监控，设置一个失效时间（比如15分钟），如果下单后一段时间仍未支付，订单就会被取消。</p>
<p>​        本文将使用<strong>FlinkCEP</strong>库来实现这个功能。</p>
<a id="more"></a>

<h4 id="二、基本需求"><a href="#二、基本需求" class="headerlink" title="二、基本需求"></a>二、基本需求</h4><p>​    1.用户下单之后，应设置订单失效时间，以提高用户支付的意愿，并降低系统风险</p>
<p>​    2.用户下单后15分钟未支付，则输出监控信息</p>
<h4 id="三、解决思路"><a href="#三、解决思路" class="headerlink" title="三、解决思路"></a>三、解决思路</h4><p>​    利用 CEP 库进行事件流的模式匹配，并设定匹配的时间间隔。</p>
<h4 id="四、具体实验步骤"><a href="#四、具体实验步骤" class="headerlink" title="四、具体实验步骤"></a>四、具体实验步骤</h4><p>我们简化数据为：用户ID（orderId Long），事件类型（eventType String）,事件时间（eventTime Long）。</p>
<p>其中时间类型包括：create 和 pay</p>
<p>我们先将事件流按照订单号orderId分流，然后定义这样的一个事件模式：在15分钟内，事件“create”与“pay”非严格连续</p>
<h6 id="1-创建maven工程"><a href="#1-创建maven工程" class="headerlink" title="1.创建maven工程"></a>1.创建maven工程</h6><p>​    可以叫做OrderTimeoutDetect。</p>
<h6 id="2-配置pom文件"><a href="#2-配置pom文件" class="headerlink" title="2.配置pom文件"></a>2.配置pom文件</h6><p>因为需要使用CEP库 所以要引入相关依赖 </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;flink-cep_2.11&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;1.9.1&lt;/version&gt;</span><br><span class="line">      &lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>其他还有Flink通用的依赖</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;1.9.1&lt;/version&gt;</span><br><span class="line">      &lt;/dependency&gt;</span><br><span class="line">      &lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;1.9.1&lt;/version&gt;</span><br><span class="line">      &lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>编译打包方式官方推荐用maven-shade-plugin。<a href="https://github.com/comsir/Flink-UserBehaviorAnalysis/blob/master/pom.xml" target="_blank" rel="noopener">完整pom链接</a></p>
<h6 id="3-定义POJO"><a href="#3-定义POJO" class="headerlink" title="3.定义POJO"></a>3.定义POJO</h6><p>定义OrderEvent <a href="https://github.com/comsir/Flink-UserBehaviorAnalysis/blob/master/OrderTimeoutDetect/src/main/java/OrderEvent.java" target="_blank" rel="noopener">完整类</a>输入的订单事件流；另外定义OrderResult <a href="https://github.com/comsir/Flink-UserBehaviorAnalysis/blob/master/OrderTimeoutDetect/src/main/java/OrderResult.java" target="_blank" rel="noopener">完整类</a>为输出显示的订单状态结果。</p>
<h6 id="4-创建主类OrderTimeout"><a href="#4-创建主类OrderTimeout" class="headerlink" title="4.创建主类OrderTimeout"></a>4.创建主类OrderTimeout</h6><p>先创建流作业的环境已经简单的配置 使用的是事件时间</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p>测试用的自定义数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;OrderEvent&gt; dataSource = env.fromCollection(Arrays.asList(</span><br><span class="line">                <span class="keyword">new</span> OrderEvent(<span class="number">1L</span>, <span class="string">"create"</span>, <span class="number">1558430842L</span>),</span><br><span class="line">                <span class="keyword">new</span> OrderEvent(<span class="number">2L</span>, <span class="string">"create"</span>, <span class="number">1558430843L</span>),</span><br><span class="line">                <span class="keyword">new</span> OrderEvent(<span class="number">2L</span>, <span class="string">"pay"</span>, <span class="number">1558430844L</span>)</span><br><span class="line">        ))</span><br></pre></td></tr></table></figure>

<p>有了数据源之后，我们需要对数据流按照orderId做keyby分流</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">KeyedStream&lt;OrderEvent, Long&gt; orderEventStream = dataSource.assignTimzestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;OrderEvent&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(OrderEvent element)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> element.getEventTime() * <span class="number">1000</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">                .keyBy(<span class="keyword">new</span> KeySelector&lt;OrderEvent, Long&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">getKey</span><span class="params">(OrderEvent value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> value.getOrderId();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br></pre></td></tr></table></figure>

<p>在做keyBy之前日常加上waterMark。因为我们这里的时间的顺序的 所以用assignTimzestampsAndWatermarks就行，生产中很少用。</p>
<p>这样就得到了一个KeyedStream，留着备用。</p>
<h6 id="5-定义Pattern-匹配模式"><a href="#5-定义Pattern-匹配模式" class="headerlink" title="5.定义Pattern 匹配模式"></a>5.定义Pattern 匹配模式</h6><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pattern&lt;OrderEvent, OrderEvent&gt; OrderPayPattern = Pattern.&lt;OrderEvent&gt;begin(<span class="string">"begin"</span>).where(<span class="keyword">new</span> SimpleCondition&lt;OrderEvent&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(OrderEvent value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.getEventType().equals(<span class="string">"create"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//非严格连续 </span></span><br><span class="line">        .followedBy(<span class="string">"follow"</span>).where(<span class="keyword">new</span> SimpleCondition&lt;OrderEvent&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(OrderEvent value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.getEventType().equals(<span class="string">"pay"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    <span class="comment">//设定超时时间</span></span><br><span class="line">    .within(Time.seconds(<span class="number">15</span>));</span><br></pre></td></tr></table></figure>

<h6 id="6-定义标签"><a href="#6-定义标签" class="headerlink" title="6.定义标签"></a>6.定义标签</h6><p>定义超时支付的输出标签，用标签sideOutput</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">OutputTag&lt;OrderResult&gt; orderTimeoutOutput = <span class="keyword">new</span> OutputTag&lt;OrderResult&gt;(<span class="string">"orderTimeout"</span>)&#123;&#125;;</span><br></pre></td></tr></table></figure>

<p><strong>特别注意：不是简单的new 一个类，是一个匿名内部类</strong></p>
<h6 id="7-数据流绑定pattern"><a href="#7-数据流绑定pattern" class="headerlink" title="7.数据流绑定pattern"></a>7.数据流绑定pattern</h6><p>将keyBy后的流绑定模式 keyedStream转换成patternStream</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">PatternStream&lt;OrderEvent&gt; patternStream = CEP.pattern(orderEventStream, OrderPayPattern);</span><br></pre></td></tr></table></figure>

<h6 id="8-获取匹配到的数据流"><a href="#8-获取匹配到的数据流" class="headerlink" title="8.获取匹配到的数据流"></a>8.获取匹配到的数据流</h6><p>从patternStream中获取匹配的流（包括超时的和正常的  等等要分别处理）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;OrderResult&gt; completedDataStream = patternStream.select(</span><br><span class="line">   <span class="comment">//第一个参数 刚定义的超时输出的标签 </span></span><br><span class="line">   orderTimeoutOutput, </span><br><span class="line">   <span class="comment">//第二个参数 处理超时的流</span></span><br><span class="line">   <span class="keyword">new</span> PatternTimeoutFunction&lt;OrderEvent, OrderResult&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> OrderResult <span class="title">timeout</span><span class="params">(Map&lt;String, List&lt;OrderEvent&gt;&gt; pattern, <span class="keyword">long</span> timeoutTimestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    Long timeoutOrderId = pattern.getOrDefault(<span class="string">"begin"</span>, <span class="keyword">null</span>).iterator().next().getOrderId();</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> OrderResult(timeoutOrderId, <span class="string">"timeout"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">     <span class="comment">//第三个参数 处理正常的流</span></span><br><span class="line">     <span class="keyword">new</span> PatternSelectFunction&lt;OrderEvent, OrderResult&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> OrderResult <span class="title">select</span><span class="params">(Map&lt;String, List&lt;OrderEvent&gt;&gt; pattern)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    Long payOrderId = pattern.getOrDefault(<span class="string">"follow"</span>, <span class="keyword">null</span>).iterator().next().getOrderId();</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> OrderResult(payOrderId, <span class="string">"success"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br></pre></td></tr></table></figure>

<h6 id="9-将completedDataStream中数据流分别输出"><a href="#9-将completedDataStream中数据流分别输出" class="headerlink" title="9.将completedDataStream中数据流分别输出"></a>9.将completedDataStream中数据流分别输出</h6><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//正常支付</span></span><br><span class="line">completedDataStream.print();</span><br><span class="line"><span class="comment">//超时支付</span></span><br><span class="line">DataStream&lt;OrderResult&gt; sideOutput = ((SingleOutputStreamOperator&lt;OrderResult&gt;) completedDataStream).getSideOutput(orderTimeoutOutput);sideOutput.print();</span><br><span class="line">env.execute(<span class="string">"order timeout job"</span>);</span><br></pre></td></tr></table></figure>

<p>完整的代码链接</p>
<p><a href="https://github.com/comsir/Flink-UserBehaviorAnalysis/tree/master/OrderTimeoutDetect" target="_blank" rel="noopener">https://github.com/comsir/Flink-UserBehaviorAnalysis/tree/master/OrderTimeoutDetect</a></p>
]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>contos7单机安装hdfs</title>
    <url>/2019/12/17/contos7%E5%8D%95%E6%9C%BA%E5%AE%89%E8%A3%85hdfs/</url>
    <content><![CDATA[<p>背景：本文介绍在centos7中安装单机版的hdfs,供学习测试hdfs用。</p>
<a id="more"></a>
<ol>
<li>VMware15 </li>
<li>centos7</li>
<li>在非root目录下操作</li>
<li>jdk1.8.0_231</li>
<li>hadoop-3.1.3</li>
</ol>
<p><strong>一、配置JDK环境</strong></p>
<p>参考上一篇文章 <a href="https://blog.csdn.net/qq_31963719/article/details/103305446" target="_blank" rel="noopener">文章链接</a></p>
<p><strong>二、配置免密登入</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">cd ~&#x2F;.ssh</span><br><span class="line">cat id_rsa.pub&gt;&gt;authorized_keys</span><br><span class="line">#如果不能绵密登录，执行以下命令</span><br><span class="line">chmod 710 authorized_keys</span><br></pre></td></tr></table></figure>
<p>测试是否配置成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[song@master ~]$ ssh localhost</span><br><span class="line">Last login: Fri Nov 29 10:04:31 2019 from 192.168.91.1</span><br></pre></td></tr></table></figure>
<p>如上情况 说明配置成功</p>
<p><strong>三、下载hadoop</strong><br>官网下载比较慢 推荐几个国内的镜像网站：(开源软件的镜像基本都有)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;mirror.bit.edu.cn&#x2F;apache&#x2F;hadoop&#x2F;common&#x2F;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;mirrors.hust.edu.cn&#x2F;apache&#x2F;hadoop&#x2F;common</span><br></pre></td></tr></table></figure>
<p>用的版本是<strong>hadoop-3.1.3.tar.gz</strong><br>用 liunx 命令 <strong>rz</strong> 上传到自己的目录/home/song/soft</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[song@master soft]$ ll</span><br><span class="line">drwxr-xr-x. 10 song song       161 Nov 28 13:34 hadoop-3.1.3</span><br><span class="line">-rw-r--r--.  1 song song 338075860 Nov 27 22:21 hadoop-3.1.3.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf hadoop-3.1.3.tar.gz</span><br></pre></td></tr></table></figure>
<p><strong>四、配置文件</strong><br>1.配置hadoop-env.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd hadoop-3.1.3</span><br><span class="line">vim etc&#x2F;hadoop&#x2F;hadoop-env.sh</span><br></pre></td></tr></table></figure>
<p>添加JDK路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;home&#x2F;song&#x2F;soft&#x2F;jdk1.8.0_231</span><br></pre></td></tr></table></figure>
<p>2.配置core-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd hadoop-3.1.3</span><br><span class="line">vim etc&#x2F;hadoop&#x2F;core-site.xml</span><br></pre></td></tr></table></figure>
<p>在&lt;configuration 中配置property</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- 指定HDFS的（namenode）的通信地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hdfs:&#x2F;&#x2F;master(自己的hostname):9000&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 指定hadoop运行时产生文件的存储路径 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;home&#x2F;song&#x2F;tmp&#x2F;hadoop&#x2F;tmp(自己的路径)&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>3.配置 hdfs-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd hadoop-3.1.3</span><br><span class="line">vim etc&#x2F;hadoop&#x2F;hdfs-site.xml</span><br></pre></td></tr></table></figure>
<p>在&lt;configuration 中配置property</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;value&gt;&#x2F;home&#x2F;song&#x2F;tmp&#x2F;hadoop&#x2F;hdfs&#x2F;name&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;value&gt;&#x2F;home&#x2F;song&#x2F;tmp&#x2F;hadoop&#x2F;hdfs&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!-- 设置hdfs副本数量 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;!--hdfs的访问权限设置为false--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.permissions&lt;&#x2F;name&gt;</span><br><span class="line">         &lt;value&gt;false&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!--web界面访问--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">         &lt;name&gt;dfs.http.address&lt;&#x2F;name&gt;</span><br><span class="line">         &lt;value&gt;192.168.91.101(自己的主机IP):50070&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!--开启webhdfs--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.webhdfs.enabled&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p><strong>五、启动hdfs</strong><br>第一次启动需要格式化namenode</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>
<p>格式化完 启动hdfs</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd hadoop-3.1.3</span><br><span class="line">.&#x2F;sbin&#x2F;start-dfs.sh</span><br></pre></td></tr></table></figure>
<p>启动成功（我的主机名叫master）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Starting namenodes on [master]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [master]</span><br></pre></td></tr></table></figure>
<p>jps查看启动的进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">3892 NameNode</span><br><span class="line">4038 DataNode</span><br><span class="line">4221 SecondaryNameNode</span><br></pre></td></tr></table></figure>
<p>最后就是在浏览器中测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;192.168.91.101(自己机器的IP):50070&#x2F;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Say hello to the world</title>
    <url>/2019/12/17/Say-hello-to-the-world/</url>
    <content><![CDATA[<p>博学之，审问之，慎思之</p>
]]></content>
  </entry>
</search>
